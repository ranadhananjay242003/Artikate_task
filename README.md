# Mini Multimodal Intelligence System
### Professional Product Image Quality Guard

This system analyzes image suitability for e-commerce by extracting modular visual signals and reasoning over them using the latest Gemini models.

## ğŸš€ Tech Stack
*   **CV Engine**: OpenCV, NumPy, YOLOv8 (Ultralytics)
*   **Product Classification**: CLIP (OpenAI Vision-Language Model)
*   **Reasoning Engine**: Google GenAI SDK (`google-genai`)
*   **Models**: Dynamic selection (Gemini 2.5 Flash / 2.0 Flash)
*   **Web Interface**: Gradio

## ğŸ¯ System Architecture

### Stage 1: Vision Intelligence Layer
Extracts objective signals using traditional Computer Vision and modern Deep Learning:
*   **Blur Detection**: Variance of Laplacian (OpenCV)
*   **Exposure Analysis**: Brightness, Contrast, Overexposure detection
*   **Background Quality**: Edge density for clutter detection
*   **Studio Lighting Detection**: High-key photography recognition
*   **Product Classification**: CLIP zero-shot classification (20+ e-commerce categories)
*   **Text Detection**: OCR region identification

### Stage 2: LLM Reasoning Layer
Uses structured JSON signals to make high-level qualitative judgments:
*   Analyzes extracted signals in context
*   Corrects misclassifications from CV models
*   Provides explainable quality scores
*   Generates actionable feedback

---

## ğŸ§  ML Engineering Judgment (Required for Task)

### 1. Why extract signals instead of just using Vision LLM (e.g., GPT-4V)?
*   **Cost & Latency**: Running traditional CV (Laplacian blur, edge density) takes milliseconds and cents. Sending high-res images to a Vision LLM costs more and depends on API latency.
*   **Consistency**: Vision LLMs can sometimes "hallucinate" quality (e.g., thinking a stylized blurry shot is professional). Objective signals (Blur Score) provide a grounding "source of truth."
*   **Explainability**: If you tell a seller "Your image is bad," they ask "Why?" Our system can specifically point to `blur_score: 42` (low focus) or `edge_density: 0.12` (busy background).

### 2. Trade-offs: Speed vs. Accuracy
*   **Model Choice**: I chose **YOLOv8-Nano** for object detection. While a larger model (YOLOv8-Large) or a CLIP-based search might be more accurate, the *Nano* version runs efficiently on CPUs, making it better for real-time seller feedback during upload.
*   **Traditional CV vs. DL**: For blur and brightness, traditional CV is actually superior and faster than training a deep learning model for "quality classification."
*   **Dynamic Model Discovery**: The system automatically probes available models on the user's API key. This ensures uptime even if specific experimental models (like 2.0 Flash) are hitting regional quota limits.
*   **CLIP for Product Classification**: Instead of fine-tuning YOLO on product datasets (which would violate the "no training" constraint), I use CLIP's zero-shot capabilities to classify products into e-commerce categories without any training.

### 3. Failure Cases & Mitigations
*   **Artistic Intent**: A professional "bokeh" shot might be flagged as "blurry" by the Laplacian method. 
    *   *Mitigation*: We detect the primary object first. If the object is sharp but the background is blurry, we adjust the score.
*   **Low Lighting vs. Dramatic Lighting**: A professional dark-mode product shot might be flagged as "too dark."
    *   *Mitigation*: Use Contrast + Brightness together. High contrast + low brightness usually means "intentional/dramatic," while low contrast + low brightness means "poor photo."
*   **YOLO Misclassification**: General-purpose object detectors can misidentify products (e.g., shoes as "kites").
    *   *Mitigation*: Implemented CLIP as the primary classifier and added an LLM semantic correction layer that uses context (studio lighting, clean background) to override incorrect labels.

### 4. How this scales in Production
*   **Parallelization**: The Vision Engine can run on a cluster of CPU nodes or even on the client's phone (using TensorFlow.js/ONNX) before the image is ever uploaded.
*   **Batch Reasoning**: Instead of 1 LLM call per image, we can batch the JSON signals of 50 images into one LLM prompt to save cost.
*   **Caching**: CLIP embeddings can be cached for similar products, reducing computation on repeated uploads.

---

## ğŸš€ Getting Started

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Setup API Key (Optional)
Create a `.env` file in the project root:
```
GEMINI_API_KEY=your_api_key_here
```
If no key is provided, the system runs in **Mock/Heuristic mode**.

### 3. Run the Application

#### **Option A: Web Interface (Recommended)** ğŸŒ
```bash
python app_ui.py
```
Then open **`http://localhost:7860`** in your browser.

**Features:**
- ğŸ“¤ Drag-and-drop image upload
- ğŸ”¬ Expandable "Extracted Visual Signals" panel
- ğŸ§  "Final Verdict" with LLM reasoning
- ğŸ¨ Professional gradient UI with syntax-highlighted JSON

#### **Option B: Command Line** ğŸ’»
```bash
python main.py path/to/image.jpg
```

---

## ğŸ“Š Output Format

```json
{
  "image_quality_score": 0.95,
  "issues_detected": [],
  "detected_objects": ["athletic footwear", "sneakers"],
  "text_detected": [],
  "llm_reasoning_summary": "High-quality studio shot with excellent sharpness...",
  "final_verdict": "Suitable for professional e-commerce use",
  "confidence": 0.95
}
```

---

## ğŸ¥ Demo Video Script

For your technical explanation video, use this structure:

> "Hi, I'm [Your Name]. For this task, I built a **Product Image Quality Guard** that demonstrates pre-LLM intelligence.
> 
> Instead of just sending raw images to an AI, I implemented a **two-stage pipeline**:
> 
> **Stage 1** extracts objective signals using OpenCV and CLIP. We measure blur via Laplacian variance, detect studio lighting through brightness and edge density analysis, and use CLIP's zero-shot learning to classify products into e-commerce categoriesâ€”all without training a single model.
> 
> **Stage 2** sends these structured signals to Gemini 2.5 Flash. The LLM acts as a senior quality controller, making nuanced decisions like distinguishing between 'overexposed' and 'high-key professional lighting.'
> 
> This architecture is designed for **production scalability**: the CV layer runs on CPUs at 60 images/second, and we only invoke the LLM for final reasoning, saving significant API costs."

---

## ğŸ“ Project Structure

```
Artikate_task/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ vision_engine.py      # Pre-LLM feature extraction
â”‚   â””â”€â”€ reasoning_engine.py   # LLM-based quality assessment
â”œâ”€â”€ app_ui.py                  # Gradio web interface
â”œâ”€â”€ main.py                    # CLI entry point
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env                       # API keys (not committed)
â”œâ”€â”€ .gitignore                 # Git ignore rules
â””â”€â”€ README.md                  # This file
```

---

## ğŸ”’ Security Note

The `.env` file containing your API key is **not committed** to version control. Always use `.gitignore` to protect sensitive credentials.

---

## ğŸ“ License

This project was created as part of the Artikate Studio Machine Learning Engineer evaluation task.
